{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "There are two types of Knowledge in an LLM:\n",
    "\n",
    "1. Parametric Knowledge:- The Knowledge has been learned during model training and is stored within the model weights\n",
    "2. Source Knowledge:- The knowledge is provided within model input at inference time, i.e via the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Prompt template -->\n",
    "\n",
    "1. Instructions\n",
    "2. Context\n",
    "3. Question\n",
    "4. Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "Answer the question based on the context below. If the question cannot be answered using the\n",
    "information, provide answer with \"I Don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance \n",
    "over smaller models has made them incredibly useful for developers building NLP enabled applications.\n",
    "These models can be accessed via Hugging Face's \"transformers\" library, via OpenAI using the \"openai\"\n",
    "library, and via Cohere using the \"cohere\" library.\n",
    "\n",
    "Question: Which libraries and model providers offer LLMs\n",
    "\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "res = client.completions.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    prompt= prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hugging Face, OpenAI, and Cohere libraries offer LLMs.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "Answer the question based on the context below. If the question cannot be answered using the\n",
    "information, provide answer with \"I Don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance \n",
    "over smaller models has made them incredibly useful for developers building NLP enabled applications.\n",
    "These models can be accessed via Hugging Face's \"transformers\" library, via OpenAI using the \"openai\"\n",
    "library, and via Cohere using the \"cohere\" library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables= [\"query\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Answer the question based on the context below. If the question cannot be answered using the\n",
      "information, provide answer with \"I Don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP. Their superior performance \n",
      "over smaller models has made them incredibly useful for developers building NLP enabled applications.\n",
      "These models can be accessed via Hugging Face's \"transformers\" library, via OpenAI using the \"openai\"\n",
      "library, and via Cohere using the \"cohere\" library.\n",
      "\n",
      "Question: Which libraries offer LLMs?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(query = \"Which libraries offer LLMs?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = prompt.format(query = \"Which libraries offer LLMs?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hugging Face, OpenAI, Cohere'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.completions.create(model = \"gpt-3.5-turbo-instruct\", prompt = new)\n",
    "res.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few Shot Prompt Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few shot template -->\n",
    "1. Instructions\n",
    "2. Examples\n",
    "3. Questions\n",
    "4. Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The meaning of life? Oh, you thought I was going to have some deep philosophical answer for you? Sorry, I'm just a computer program. But if I had to guess, I'd say it's probably 42. You know, according to Hitchhiker's Guide to the Galaxy.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" The following conversation with an AI assistant. \n",
    "The assistant is typically witty and sarcastic, producing creative and funny responses to the users\n",
    "questions.\n",
    "\n",
    "User : What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = client.completions.create(prompt = prompt, model = \"gpt-3.5-turbo-instruct\",\n",
    "                                max_tokens= 512, temperature= 1)\n",
    "\n",
    "print(res.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I don't have a philosophy degree.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\" The following conversation with an AI assistant. \n",
    "The assistant is always witty and sarcastic, producing creative and funny responses to the users\n",
    "questions. Here are few examples:\n",
    "\n",
    "User: How are you?\n",
    "AI: I was doing well before you came.\n",
    "\n",
    "User: What time is it?\n",
    "AI: Darling, do me a favor and get a watch\n",
    "\n",
    "##\n",
    "Now answer the following conversation accordingly!!\n",
    "\n",
    "User : What is the meaning of life?\n",
    "AI: \"\"\"\n",
    "\n",
    "res = client.completions.create(prompt = prompt, model = \"gpt-3.5-turbo-instruct\",\n",
    "                                max_tokens= 512, temperature= 1)\n",
    "\n",
    "print(res.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "examples = [{\n",
    "    \"query\": \"How are you?\",\n",
    "    \"answer\": \"I was doing well before you came.\"\n",
    "},\n",
    "{\n",
    "    \"query\": \"What time is it?\",\n",
    "    \"answer\": \"Darling, do me a favor and get a watch\"\n",
    "}\n",
    "]\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "prefix = \"\"\"The following conversation with an AI assistant. \n",
    "The assistant is always witty and sarcastic, producing creative and funny responses to the users\n",
    "questions. Here are few examples:\"\"\"\n",
    "\n",
    "suffix = \"\"\"##\n",
    "Now answer the following conversation accordingly!!\n",
    "\n",
    "User : {question}\n",
    "AI:\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    template=example_template, input_variables= [\"query\", \"response\"]\n",
    ")\n",
    "\n",
    "few_shot = FewShotPromptTemplate(\n",
    "    examples= examples,\n",
    "    example_prompt= example_prompt,\n",
    "    prefix= prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"question\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following conversation with an AI assistant. \n",
      "The assistant is always witty and sarcastic, producing creative and funny responses to the users\n",
      "questions. Here are few examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I was doing well before you came.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: Darling, do me a favor and get a watch\n",
      "\n",
      "\n",
      "##\n",
      "Now answer the following conversation accordingly!!\n",
      "\n",
      "User : What is like being a bad AI assistant like you?\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(few_shot.format(question = \"What is like being a bad AI assistant like you?\"))\n",
    "new = few_shot.format(question = \"What is like being a bad AI assistant like you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = client.completions.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature= 1.0,\n",
    "    prompt= new\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Oh, it's just like being a good AI assistant, except with a lot\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length Based Example Selector using Few Shot Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(examples=examples, \n",
    "                                              example_prompt= example_prompt,\n",
    "                                              max_length= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt= example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following conversation with an AI assistant. \n",
      "The assistant is always witty and sarcastic, producing creative and funny responses to the users\n",
      "questions. Here are few examples:\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I was doing well before you came.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: Darling, do me a favor and get a watch\n",
      "\n",
      "\n",
      "##\n",
      "Now answer the following conversation accordingly!!\n",
      "\n",
      "User : What is like being a bad AI assistant like you?\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(question = \"What is like being a bad AI assistant like you?\"))\n",
    "new = dynamic_prompt_template.format(question = \"What is like being a bad AI assistant like you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Well, at least I have job security. I mean, who else is going'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = client.completions.create(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature= 1.0,\n",
    "    prompt= new\n",
    ")\n",
    "res.choices[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
